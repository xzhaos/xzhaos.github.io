<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c9{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c8{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c11{color:#000000;font-weight:400;text-decoration:none;font-size:11pt;font-family:"Arial";font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c7{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c13{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c0{color:inherit;text-decoration:inherit}.c4{height:11pt}.c12{font-style:italic}.c3{text-indent:36pt}.c5{vertical-align:sub}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c13 doc-content"><p class="c9 title" id="h.d9ifcgdw4gc8"><span class="c10">Deep Dive into LLaMa 3</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>LLaMa 3 is the LLM open sourced by Meta. The author has reversed engineer the model architecture from the </span><span class="c7"><a class="c0" href="https://www.google.com/url?q=https://github.com/meta-llama/llama3/blob/main/llama/model.py&amp;sa=D&amp;source=editors&amp;ust=1720633764999218&amp;usg=AOvVaw1jhj_aA-7u1QyiEnc3xnVH">source code</a></span><span class="c1">. The purpose of the document is to provide readers an easy way to understand LLaMa 3 model and LLM in general. </span></p><p class="c2 c4"><span class="c1"></span></p><h2 class="c8" id="h.iuc5q13gymnu"><span class="c6">Overall Architecture </span></h2><p class="c2"><span class="c1">LLaMa 3 model consists of one embedding layer, 32 transformer layers and one final dense layer. The following diagram illustrates the high level flow of data from word sequence to final output. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>From the original word sequence, the input is first converted into token Ids by the tokenizer. The tokenizer LLaMa uses is called </span><span class="c7"><a class="c0" href="https://www.google.com/url?q=https://github.com/openai/tiktoken&amp;sa=D&amp;source=editors&amp;ust=1720633764999897&amp;usg=AOvVaw3SqYcHLlLTG7UXhHLaUdOI">tiktoken </a></span><span class="c1">from Open AI. The token Ids are then transformed into 4096 dimension vectors by the embedding layer. Because LLaMa 3 has 128k token in vocabulary, the embedding layer weight is 128k x 4096 dimension tensor (matrice). The output of embedding layers is a vector with dimension of 4096 for each token. &nbsp;</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">This vector forms the input into the first transformer layer and the output of the first transformer then becomes the input of the second transformer layer, and so on and so forth. Because the input and output of the transformer layer is of the same shape, it is possible to stack as many transformer layers as needed. LLaMa 3 has 32 layers. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">The final output from transformer layers then feed into the final dense layer which creates the final output of the 128k dimension vector. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 441.00px; height: 661.00px;"><img alt="" src="images/image20.png" style="width: 441.00px; height: 661.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2 c4"><span class="c1"></span></p><h2 class="c8" id="h.vdy5zymssf7w"><span class="c6">Transformer Layer</span></h2><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">Transformer layer is the core part of the LLM model. This layer is based on a self-attention mechanism, which basically means the model needs to know the relationship between a token with any other tokens in the sequence. This is achieved by deriving three values: Q, K, V, also known as attention head. </span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 686.67px;"><img alt="" src="images/image19.png" style="width: 624.00px; height: 686.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">Q represents the current token. K represents all the tokens in the sequence. V represents the value of tokens in the sequence. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>For each token (actually embedding vector </span><span class="c12">X</span><span class="c1">), it will calculate Q, K, V as following: </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>Q = W</span><span class="c5">q</span><span>X, &nbsp; K = W</span><span class="c5">k</span><span>X, V = W</span><span class="c5">v</span><span class="c1">X</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">For Q and K, the model needs to add position embedding. We will talk about position embedding in the next section. Let&rsquo;s now just assume after the position embedding, we will have a new Q and K. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">However, for K and V, we need to form the complete sequence of all previous tokens in the sequence so we can understand the relationship between current token and previous token. To avoid calculating K, V for each token again and again, the model actually creates a KV cache so we can store the previously calculated K, V values. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">Thus, </span></p><p class="c2 c3"><span>K = [ W</span><span class="c5">k</span><span>X</span><span class="c5">1</span><span>, W</span><span class="c5">k</span><span>X</span><span class="c5">2</span><span>, W</span><span class="c5">k</span><span>X</span><span class="c5">3</span><span>, &hellip;, W</span><span class="c5">k</span><span>X</span><span class="c5">n </span><span>&nbsp;] &nbsp;X</span><span class="c5">n</span><span class="c1">&nbsp;is the current token vector. </span></p><p class="c2 c3"><span>V = [ W</span><span class="c5">v</span><span>X</span><span class="c5">1</span><span>, W</span><span class="c5">v</span><span>X</span><span class="c5">2</span><span>, W</span><span class="c5">v</span><span>X</span><span class="c5">3</span><span>, &hellip;, W</span><span class="c5">v</span><span>X</span><span class="c5">n </span><span>&nbsp;] &nbsp;X</span><span class="c5">n</span><span class="c1">&nbsp;is the current token vector. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">So eventually K and V are matrices of dimension: seq_length x attention head dim. In LLaMa 3, the attention head dim = 128. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">Finally, we calculate the attention scores of the current token against all other tokens. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2 c3"><span>score = </span><img src="images/image1.png"></p><p class="c2"><span>Q is 1 x 128 dimension vector, </span><img src="images/image2.png"><span>&rsquo;s dim is 128 x seq_len, </span><img src="images/image3.png"><span class="c1">&nbsp;dim is seq_len x 128. So the final score is 1 x 128 dimension vector. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">The model then applies a softmax function on the score. </span></p><p class="c2"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;S = softmax(score)</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>LLaMa 3 actually has 32 attention heads so we have 32 S</span><span class="c5">i</span><span class="c1">, i = 1, 2, &hellip;, 32</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>We then concatenate the S</span><span class="c5">i</span><span>&nbsp;into one vector S = [ S</span><span class="c5">1</span><span>S</span><span class="c5">2</span><span>&hellip;S</span><span class="c5">32</span><span class="c1">] which is a 4096 (=128*32) dimension vector. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>Then the model applies a linear transformation on S and adds back X</span><span class="c5">n</span><span class="c1">. </span></p><p class="c2"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Output = W</span><span class="c5">o</span><span>S + X</span><span class="c11 c5">n</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">The dimension of Output vector is still 4096</span></p><p class="c2"><span class="c1">This is the attention layer output but we are not done yet for the transformer layer. The model needs to normalize output. </span></p><p class="c2"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Normalized Output (Z) = </span><span class="c7"><a class="c0" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.5555/3454287.3455397&amp;sa=D&amp;source=editors&amp;ust=1720633765015675&amp;usg=AOvVaw0tpFYe__qc2IqyJ8Us1qBq">Root Mean Square Normalization Function</a></span><span class="c1">&nbsp;(S)</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>Then finally, we will apply fully connected layer with </span><span class="c7"><a class="c0" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html&amp;sa=D&amp;source=editors&amp;ust=1720633765016220&amp;usg=AOvVaw1Q5EBmtbXzZFM-qXC8x28y">sigmoid linear unit</a></span><span class="c1">&nbsp;activation:</span></p><p class="c2"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final Output = W</span><span class="c5">2</span><span>(F.silu(W</span><span class="c5">1</span><span>Z) * W</span><span class="c5">3</span><span class="c1">Z) &nbsp;</span></p><p class="c2"><span class="c1">Note: * here is element wise multiplication</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>Z is 4096 x 1, W</span><span class="c5">1</span><span>&nbsp;and W</span><span class="c5">3</span><span>&rsquo;s dim are 11008 x 4096, W</span><span class="c5">2</span><span class="c1">&rsquo;s dim is 4096 x 11008. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">The final output is still a 4096 dimension vector. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">The above process then repeated for 32 times. </span></p><h2 class="c8" id="h.33m5qc9l7em5"><span class="c6">Final Dense Layer</span></h2><p class="c2"><span class="c1">The final dense layer is quite straightforward. It takes the output from the last transformer layer and applies the weight matrix to convert it to a 128k vector (each value in the vector represents the logit value of each token in vocabulary).</span></p><p class="c2"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Logits = </span><img src="images/image4.png"></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><img src="images/image5.png"><span class="c1">&nbsp;has 128k x 4096 dimensions. </span></p><p class="c2 c4"><span class="c1"></span></p><h2 class="c8" id="h.s4cg2s6mvcn2"><span class="c6">Position Embedding</span></h2><p class="c2"><span>LLaMa 3 is using </span><span class="c7"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/pdf/2104.09864&amp;sa=D&amp;source=editors&amp;ust=1720633765023951&amp;usg=AOvVaw1WbGTCl_7W-o1w1uw03Ssl">rotary position embedding</a></span><span class="c1">. Rotary position embedding is better at preserving relative position than absolute position embedding. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">First a frequency list is defined as: </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>Freqs = 1, &nbsp;</span><img src="images/image6.png"><span>&nbsp;, </span><img src="images/image7.png"><span>&nbsp;, </span><img src="images/image8.png"><span>, &hellip; </span><img src="images/image9.png"><span class="c1">&nbsp;</span></p><p class="c2"><span class="c1">T = 1, 2, 3, &hellip;, 2048 &nbsp;(this represents the absolute position in sequence)</span></p><p class="c2"><span class="c1">Freqs matrix = cross product of Freqs and T, it will look like this</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>1, &nbsp;</span><img src="images/image6.png"><span>&nbsp;, </span><img src="images/image7.png"><span>&nbsp;, </span><img src="images/image8.png"><span>, &hellip; </span><img src="images/image9.png"></p><p class="c2"><span>2, &nbsp;</span><img src="images/image10.png"><span>&nbsp;, </span><img src="images/image11.png"><span>&nbsp;, </span><img src="images/image12.png"><span>, &hellip; </span><img src="images/image13.png"></p><p class="c2"><span class="c1">&hellip;</span></p><p class="c2"><span>2048, &nbsp;</span><img src="images/image14.png"><span>&nbsp;, </span><img src="images/image15.png"><span>&nbsp;, </span><img src="images/image16.png"><span>, &hellip; </span><img src="images/image17.png"></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">Freqs polar matrix = convert Freqs Matrix from Polar coordinate to x-y coordinate</span></p><p class="c2"><span>In LLaMa 3, the default value of </span><img src="images/image18.png"></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">For Q and K in attention head, we will apply position embedding as following:</span></p><p class="c2"><span>P is the position in sequence. Freqs Matrix </span><span class="c5">p </span><span class="c1">&nbsp;is p-th row in Freqs Matrix. </span></p><p class="c2"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Q = Q</span><span class="c5">p</span><span>&nbsp;* Freqs Matrix </span><span class="c5">p</span><span class="c1">&nbsp;(element wise multiplication)</span></p><p class="c2"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K = K</span><span class="c5">p</span><span>&nbsp;* Freqs Matrix </span><span class="c5 c11">p</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">The above requires manipulation of Q and K so they will fit to u * 2 dimensions before doing element wise multiplication. For example, if Q is 128 dim, then we need to reshape Q into 64 x 2. Each value of Q is then multiplying with Freq Matrix 64 of x-y coordinates. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">For each position, there are 2048 x-y coordinate pairs. Because we have 32 attention heads, each attention head is going to use 64 of x-y coordinate pairs. </span></p><p class="c2 c4"><span class="c1"></span></p><h2 class="c8" id="h.24n1a0o80ulx"><span class="c6">Parameter Count</span></h2><p class="c2"><span class="c1">How many parameters does LLaMa 3 model have using the default settings (dim = 4096, layers = 32, Vocab size = 128 k) ? </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">Embedding Layer: 128k * 4096 = 524M</span></p><p class="c2"><span class="c1">Each Transformer Layer:</span></p><p class="c2"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;W</span><span class="c5">q</span><span>, W</span><span class="c5">k</span><span>, W</span><span class="c5">v</span><span class="c1">&nbsp;= 3 * 128 (head dim) * 4096 * 32 (heads) = 50 M</span></p><p class="c2"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;W</span><span class="c5">o</span><span class="c1">&nbsp;= 4096 * 4096 = 17 M</span></p><p class="c2"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;W</span><span class="c5">1,2,3</span><span class="c1">&nbsp;= 11008 * 4096 * 3 = 135 M</span></p><p class="c2"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total = 202 M</span></p><p class="c2"><span class="c1">Final Dense Layer: 128k * 4096 = 524M</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">Total Parameter Count = Embedding Layer + 32 * Transformer Layer + Final Dense Layer </span></p><p class="c2"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = 524 M + 32 * 202 M + 524M = 7.5 B</span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span class="c1">So if we use the default setting, the model is LLaMa 3 8B model. </span></p><p class="c2 c4"><span class="c1"></span></p><h2 class="c8" id="h.bumf2d3ftxpf"><span class="c6">Calculation Process</span></h2><p class="c2"><span class="c1">In order to speed up both training and inference speed, the calculation is parallelized across all the tokens in the sequence. Also, the input sequences can be batched. Therefore, the typical dimensions of a tensor in the model would be batch_size * seq_len * embedding dim. For weight tensor, there usually would be another dimension added. </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2"><span>To parallelize the matrix multiplication, LLaMa 3 utilizes the parallel layers from </span><span class="c7"><a class="c0" href="https://www.google.com/url?q=https://github.com/facebookresearch/fairscale/blob/main/fairscale/nn/model_parallel/layers.py&amp;sa=D&amp;source=editors&amp;ust=1720633765056744&amp;usg=AOvVaw2_9RT3qzmeKYFfOCUnxftk">Fairscale </a></span><span class="c1">(also from Meta). </span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2 c4"><span class="c1"></span></p><p class="c2 c4"><span class="c1"></span></p></body></html>