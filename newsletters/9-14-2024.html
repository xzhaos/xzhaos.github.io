<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, minimal-ui">
    <title>Xu&#x27;s AI Newsletter Sep 14, 2024</title>
    <link type="text/css" rel="stylesheet" href="assets/css/github-markdown.css">
    <link type="text/css" rel="stylesheet" href="assets/css/pilcrow.css">
    <link type="text/css" rel="stylesheet" href="assets/css/hljs-github.min.css"/>
  </head>
  <body>
    <article class="markdown-body"><h1 id="xus-ai-newsletter-sep-14-2024"><a class="header-link" href="#xus-ai-newsletter-sep-14-2024"></a>Xu&#39;s AI Newsletter Sep 14, 2024</h1>
<h2 id="user-confused-when-ai-unexpectedly-starts-sobbing-out-loud"><a class="header-link" href="#user-confused-when-ai-unexpectedly-starts-sobbing-out-loud"></a>User Confused When AI Unexpectedly Starts Sobbing Out Loud</h2>
<p>In a bizarre video, an AI music generator appears to be sobbing like a human, surprising the Reddit user who posted it.</p>
<p>Posted to r/SunoAI, a subreddit dedicated to the music-generating software of the same name, u/BloodMossHunter posted a 24-second clip showing the AI sounding like it&#39;s crying, which doesn&#39;t seem to have been part of the user&#39;s prompt.</p>
<p>As other users noted, emotional-sounding outbursts and other creepy bits of audio appearing randomly at the ends of these AI-generated songs seem — terrifyingly, perhaps, depending on your perspective — to be a fairly common occurrence with the software.</p>
<p><a href="https://futurism.com/suno-music-ai-sobbing">https://futurism.com/suno-music-ai-sobbing</a></p>
<h3 id="editors-note"><a class="header-link" href="#editors-note"></a>Editor&#39;s Note</h3>
<p>Despite prompt engineering, we sometimes still cannot control what is being generated. This is a potential risk if we allow Gen AI to make decision without human monitoring. </p>
<h2 id="ai2s-new-model-aims-to-be-open-and-powerful-yet-cost-effective"><a class="header-link" href="#ai2s-new-model-aims-to-be-open-and-powerful-yet-cost-effective"></a>AI2’s new model aims to be open and powerful yet cost effective</h2>
<p>The Allen Institute for AI (AI2), in collaboration with Contextual AI, released a new open-source model that hopes to answer the need for a large language model (LLM) that is both a strong performer and cost-effective. </p>
<p>The new model, which it calls OLMoE, leverages a sparse mixture of experts (MoE) architecture. It has 7 billion parameters but uses only 1 billion parameters per input token. It has two versions: OLMoE-1B-7B, which is more general purpose and OLMoE-1B-7B-Instruct for instruction tuning. </p>
<p><a href="https://venturebeat.com/ai/ai2s-new-model-aims-to-be-open-and-powerful-yet-cost-effective/">https://venturebeat.com/ai/ai2s-new-model-aims-to-be-open-and-powerful-yet-cost-effective/</a></p>
<h3 id="editors-note-1"><a class="header-link" href="#editors-note-1"></a>Editor&#39;s Note</h3>
<p>Glad to see another open source model for MoE architecture. </p>
<h2 id="roblox-announces-ai-tool-for-generating-3d-game-worlds-from-text"><a class="header-link" href="#roblox-announces-ai-tool-for-generating-3d-game-worlds-from-text"></a>Roblox announces AI tool for generating 3D game worlds from text</h2>
<p>On Friday, Roblox announced plans to introduce an open source generative AI tool that will allow game creators to build 3D environments and objects using text prompts, reports MIT Tech Review. The feature, which is still under development, may streamline the process of creating game worlds on the popular online platform, potentially opening up more aspects of game creation to those without extensive 3D design skills.</p>
<p>Roblox has not announced a specific launch date for the new AI tool, which is based on what it calls a &quot;3D foundational model.&quot; The company shared a demo video of the tool where a user types, &quot;create a race track,&quot; then &quot;make the scenery a desert,&quot; and the AI model creates a corresponding model in the proper environment.</p>
<p><a href="https://arstechnica.com/information-technology/2024/09/open-source-roblox-tool-will-allow-3d-world-creation-from-text-prompts/">https://arstechnica.com/information-technology/2024/09/open-source-roblox-tool-will-allow-3d-world-creation-from-text-prompts/</a></p>
<h3 id="editors-note-2"><a class="header-link" href="#editors-note-2"></a>Editor&#39;s Note</h3>
<p>Looking forward to see the capabilities of this tool. could be pretty interesting. </p>
<h2 id="amd-is-turning-its-back-on-flagship-gaming-gpus-to-chase-ai-first"><a class="header-link" href="#amd-is-turning-its-back-on-flagship-gaming-gpus-to-chase-ai-first"></a>AMD is turning its back on flagship gaming GPUs to chase AI first</h2>
<p>AMD is saying the quiet part out loud: it’s now prioritizing AI chips ahead of flagship GPUs for gamers. The company’s just laid out a new business strategy, where it will merge its RDNA gaming graphics and CNDA data center efforts into a single universal “UDNA” that’s aimed at AI first.</p>
<p>In two interviews with Tom’s Hardware (you’ll definitely want to read both), AMD computing and graphics boss Jack Huynh doesn’t beat around the bush. With gaming graphics, he explains, the goal is now building scale and market share at lower price points — not the “King of the Hill” flagship GPUs that haven’t convinced enough buyers to leave Nvidia behind.</p>
<p><a href="https://www.theverge.com/2024/9/9/24240173/amd-udna-gpu-ai-gaming-rdna-cdna-jack-huynh">https://www.theverge.com/2024/9/9/24240173/amd-udna-gpu-ai-gaming-rdna-cdna-jack-huynh</a></p>
<h3 id="editors-note-3"><a class="header-link" href="#editors-note-3"></a>Editor&#39;s Note</h3>
<p>Hope AMD is successful to challenge Nvidia like what they did in CPU space with Intel. Consumers will benefit from the competition. </p>
<h2 id="jensen-huang-says-nvidia-chips-are-in-high-demand-people-emotional-its-tense"><a class="header-link" href="#jensen-huang-says-nvidia-chips-are-in-high-demand-people-emotional-its-tense"></a>Jensen Huang says Nvidia chips are in high demand: ‘People emotional, it&#39;s tense’</h2>
<p>Nvidia CEO Jensen Huang said that shortage of his company&#39;s products is making customers &quot;emotional&quot; owing to which things are tense. The tech boss said the demand for Nvidia&#39;s products is so high because everyone wants to get them first which is making things tense with customers despite the company&#39;s best efforts.</p>
<p>At a Goldman Sachs tech conference in San Francisco, Jensen Huang said, &quot;We have a lot of people on our shoulders, and everybody is counting on us. Demand is so great that delivery of our components, our technology, infrastructure, and software is really emotional for people.&quot;</p>
<p><a href="https://www.hindustantimes.com/business/jensen-huang-says-nvidia-chips-are-in-high-demand-people-emotional-its-tense-101726200740972.html">https://www.hindustantimes.com/business/jensen-huang-says-nvidia-chips-are-in-high-demand-people-emotional-its-tense-101726200740972.html</a></p>
<h3 id="editors-note-4"><a class="header-link" href="#editors-note-4"></a>Editor&#39;s Note</h3>
<p>For Nvidia, this is a good problem to have. For rest of tech industry, not so good. Nvidia is eating all the cakes right now. </p>
<h2 id="openai-announces-a-new-ai-model-code-named-strawberry-that-solves-difficult-problems-step-by-step"><a class="header-link" href="#openai-announces-a-new-ai-model-code-named-strawberry-that-solves-difficult-problems-step-by-step"></a>OpenAI Announces a New AI Model, Code-Named Strawberry, That Solves Difficult Problems Step by Step</h2>
<p>OpenAI made the last big breakthrough in artificial intelligence by increasing the size of its models to dizzying proportions, when it introduced GPT-4 last year. The company today announced a new advance that signals a shift in approach—a model that can “reason” logically through many difficult problems and is significantly smarter than existing AI without a major scale-up.</p>
<p>The new model, dubbed OpenAI o1, can solve problems that stump existing AI models, including OpenAI’s most powerful existing model, GPT-4o. Rather than summon up an answer in one step, as a large language model normally does, it reasons through the problem, effectively thinking out loud as a person might, before arriving at the right result.</p>
<p><a href="https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/">https://www.wired.com/story/openai-o1-strawberry-problem-reasoning/</a></p>
<h3 id="editors-note-5"><a class="header-link" href="#editors-note-5"></a>Editor&#39;s Note</h3>
<p>I can&#39;t find any technical paper on how the model does the reasoning except high level sketch described in <a href="https://openai.com/index/learning-to-reason-with-llms/">https://openai.com/index/learning-to-reason-with-llms/</a>. OpenAI is better to be renamed as ClosedAI. </p>
<h2 id="the-godmother-of-ai-wants-everyone-to-be-a-world-builder"><a class="header-link" href="#the-godmother-of-ai-wants-everyone-to-be-a-world-builder"></a>The Godmother of AI Wants Everyone to Be a World Builder</h2>
<p>According to market-fixated tech pundits and professional skeptics, the artificial intelligence bubble has popped, and winter’s back. Fei-Fei Li isn’t buying that. In fact, Li—who earned the sobriquet the “godmother of AI”—is betting on the contrary. She’s on a part-time leave from Stanford University to cofound a company called World Labs. While current generative AI is language-based, she sees a frontier where systems construct complete worlds with the physics, logic, and rich detail of our physical reality. It’s an ambitious goal, and despite the dreary nabobs who say progress in AI has hit a grim plateau, World Labs is on the funding fast track. The startup is perhaps a year away from having a product—and it’s not clear at all how well it will work when and if it does arrive—but investors have pitched in $230 million and are reportedly valuing the nascent startup at a billion dollars.</p>
<p><a href="https://www.wired.com/story/plaintext-the-godmother-of-ai-wants-everyone-to-be-a-world-builder/">https://www.wired.com/story/plaintext-the-godmother-of-ai-wants-everyone-to-be-a-world-builder/</a></p>
<h3 id="editors-note-6"><a class="header-link" href="#editors-note-6"></a>Editor&#39;s Note</h3>
<p>The trend is using AI to interact with physical world. Looking forward to see breakthrough there. </p>
<h2 id="google-rolls-out-voice-powered-ai-chat-to-the-android-masses"><a class="header-link" href="#google-rolls-out-voice-powered-ai-chat-to-the-android-masses"></a>Google rolls out voice-powered AI chat to the Android masses</h2>
<p>On Thursday, Google made Gemini Live, its voice-based AI chatbot feature, available for free to all Android users. The feature allows users to interact with Gemini through voice commands on their Android devices. That&#39;s notable because competitor OpenAI&#39;s Advanced Voice Mode feature of ChatGPT, which is similar to Gemini Live, has not yet fully shipped.</p>
<p>Google unveiled Gemini Live during its Pixel 9 launch event last month. Initially, the feature was exclusive to Gemini Advanced subscribers, but now it&#39;s accessible to anyone using the Gemini app or its overlay on Android.</p>
<p><a href="https://arstechnica.com/information-technology/2024/09/google-rolls-out-voice-powered-ai-chat-to-the-masses/">https://arstechnica.com/information-technology/2024/09/google-rolls-out-voice-powered-ai-chat-to-the-masses/</a></p>
<h3 id="editors-note-7"><a class="header-link" href="#editors-note-7"></a>Editor&#39;s Note</h3>
<p>Google seems to make more progress faster now than a year ago in AI space. </p>
    </article>
  </body>
</html>
