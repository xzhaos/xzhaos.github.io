<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, minimal-ui">
    <title>Xu&#x27;s AI Newsletter Oct 19, 2024</title>
    <link type="text/css" rel="stylesheet" href="assets/css/github-markdown.css">
    <link type="text/css" rel="stylesheet" href="assets/css/pilcrow.css">
    <link type="text/css" rel="stylesheet" href="assets/css/hljs-github.min.css"/>
  </head>
  <body>
    <article class="markdown-body"><h1 id="xus-ai-newsletter-oct-19-2024"><a class="header-link" href="#xus-ai-newsletter-oct-19-2024"></a>Xu&#39;s AI Newsletter Oct 19, 2024</h1>
<h2 id="cheap-ai-video-scraping-can-now-extract-data-from-any-screen-recording"><a class="header-link" href="#cheap-ai-video-scraping-can-now-extract-data-from-any-screen-recording"></a>Cheap AI “video scraping” can now extract data from any screen recording</h2>
<p>Recently, AI researcher Simon Willison wanted to add up his charges from using a cloud service, but the payment values and dates he needed were scattered among a dozen separate emails. Inputting them manually would have been tedious, so he turned to a technique he calls &quot;video scraping,&quot; which involves feeding a screen recording video into an AI model, similar to ChatGPT, for data extraction purposes.</p>
<p>What he discovered seems simple on its surface, but the quality of the result has deeper implications for the future of AI assistants, which may soon be able to see and interact with what we&#39;re doing on our computer screens.</p>
<p>&quot;The other day I found myself needing to add up some numeric values that were scattered across twelve different emails,&quot; Willison wrote in a detailed post on his blog. He recorded a 35-second video scrolling through the relevant emails, then fed that video into Google&#39;s AI Studio tool, which allows people to experiment with several versions of Google&#39;s Gemini 1.5 Pro and Gemini 1.5 Flash AI models.</p>
<p>Willison then asked Gemini to pull the price data from the video and arrange it into a special data format called JSON (JavaScript Object Notation) that included dates and dollar amounts. The AI model successfully extracted the data, which Willison then formatted as CSV (comma-separated values) table for spreadsheet use. After double-checking for errors as part of his experiment, the accuracy of the results—and what the video analysis cost to run—surprised him.</p>
<p>[<a href="https://arstechnica.com/ai/2024/10/cheap-ai-video-scraping-can-now-extract-data-from-any-screen-recording/]">https://arstechnica.com/ai/2024/10/cheap-ai-video-scraping-can-now-extract-data-from-any-screen-recording/]</a></p>
<h3 id="editors-note"><a class="header-link" href="#editors-note"></a>Editor&#39;s Note</h3>
<p>LLMs these days are really good at cognitive tasks. Extracting content is one of the best use cases for LLMs. </p>
<h2 id="apple-study-exposes-deep-cracks-in-llms-reasoning-capabilities"><a class="header-link" href="#apple-study-exposes-deep-cracks-in-llms-reasoning-capabilities"></a>Apple study exposes deep cracks in LLMs’ “reasoning” capabilities</h2>
<p>For a while now, companies like OpenAI and Google have been touting advanced &quot;reasoning&quot; capabilities as the next big step in their latest artificial intelligence models. Now, though, a new study from six Apple engineers shows that the mathematical &quot;reasoning&quot; displayed by advanced large language models can be extremely brittle and unreliable in the face of seemingly trivial changes to common benchmark problems.</p>
<p>The fragility highlighted in these new results helps support previous research suggesting that LLMs use of probabilistic pattern matching is missing the formal understanding of underlying concepts needed for truly reliable mathematical reasoning capabilities. &quot;Current LLMs are not capable of genuine logical reasoning,&quot; the researchers hypothesize based on these results. &quot;Instead, they attempt to replicate the reasoning steps observed in their training data.&quot;</p>
<p>[<a href="https://arstechnica.com/ai/2024/10/llms-cant-perform-genuine-logical-reasoning-apple-researchers-suggest/]">https://arstechnica.com/ai/2024/10/llms-cant-perform-genuine-logical-reasoning-apple-researchers-suggest/]</a></p>
<h3 id="editors-note-1"><a class="header-link" href="#editors-note-1"></a>Editor&#39;s Note</h3>
<p>Whether LLMs can reason is an active research area. We are still not sure if LLMs can actually reason or just perform very advanced pattern matching. </p>
<h2 id="nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4no-big-launch-just-big-results"><a class="header-link" href="#nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4no-big-launch-just-big-results"></a>Nvidia just dropped a new AI model that crushes OpenAI’s GPT-4—no big launch, just big results</h2>
<p>Nvidia quietly unveiled a new artificial intelligence model on Tuesday that outperforms offerings from industry leaders OpenAI and Anthropic, marking a significant shift in the company’s AI strategy and potentially reshaping the competitive landscape of the field.</p>
<p>The model, named Llama-3.1-Nemotron-70B-Instruct, appeared on the popular AI platform Hugging Face without fanfare, quickly drawing attention for its exceptional performance across multiple benchmark tests.</p>
<p>[<a href="https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/]">https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/]</a></p>
<h3 id="editors-note-2"><a class="header-link" href="#editors-note-2"></a>Editor&#39;s Note</h3>
<p>I am convinced that open source models will eventually trump the proprietary models just like open source software did in the last couple of decades. Given the near monopoly of hardward capabilities, Nvidia will be the driving force in open source&#39;s eventual dominance. It is also good for Nvidia: more open source models lead to more people training or hosting these models and lead to more customers for Nvidia. </p>
<h2 id="openais-latest-release-swarm-brings-ai-agents-to-life--heres-why-it-matters"><a class="header-link" href="#openais-latest-release-swarm-brings-ai-agents-to-life--heres-why-it-matters"></a>OpenAI’s Latest Release: ‘Swarm’ Brings AI Agents to Life — Here’s Why It Matters</h2>
<p>Inthe fast-paced world of AI, every new development feels like it’s pushing us closer to what was once considered science fiction. This time, OpenAI has once again changed the game by releasing its latest product, Swarm.</p>
<p>Swarm isn’t just another AI upgrade. It’s an entirely new approach to AI, giving us a glimpse of the future where AI agents collaborate, communicate, and automate tasks without human intervention.</p>
<p>If you’ve been keeping up with the exciting breakthroughs in generative AI and large language models (LLMs), you’ll know that agentic AI is the next big leap. And if OpenAI is leading the charge, you can bet this is something you don’t want to miss.</p>
<p>[<a href="https://ai.gopubby.com/openais-latest-release-swarm-brings-ai-agents-to-life-here-s-why-it-matters-efeb517f464e]">https://ai.gopubby.com/openais-latest-release-swarm-brings-ai-agents-to-life-here-s-why-it-matters-efeb517f464e]</a></p>
<h3 id="editors-note-3"><a class="header-link" href="#editors-note-3"></a>Editor&#39;s Note</h3>
<p>If anyone has tried this, plese let me know what you think of Swarm. <a href="https://github.com/openai/swarm">https://github.com/openai/swarm</a></p>
<h2 id="the-next-wave-of-ai-wont-be-driven-by-llms-heres-what-investors-should-focus-on-instead"><a class="header-link" href="#the-next-wave-of-ai-wont-be-driven-by-llms-heres-what-investors-should-focus-on-instead"></a>The next wave of AI won’t be driven by LLMs. Here’s what investors should focus on instead</h2>
<p>Apple just published a paper that subtly acknowledges what many in the artificial intelligence (AI) community have been hinting at for some time: Large language models (LLMs) are approaching their limits. These systems—like OpenAI’s GPT-4—have dazzled the world with their ability to generate human-like text, answer complex questions, and assist in tasks across industries. But behind the curtain of excitement, it’s becoming clear that we may be hitting a plateau. This isn’t just Apple’s perspective. AI experts like Gary Marcus have been sounding the alarm for years, warning that LLMs, despite their brilliance, are running into significant limitations.</p>
<p>Yet, despite these warnings, venture capitalists (VCs) have been pouring billions into LLM startups like lemmings heading off a cliff. The allure of LLMs, driven by the fear of missing out on the next AI gold rush, has led to a frenzy of investment. VCs are chasing the hype without fully appreciating the fact that LLMs may have already peaked. And like lemmings, most of these investors will soon find themselves tumbling off the edge, losing their me-too investments as the technology hits its natural limits.</p>
<p>One of the most promising areas of AI development is neurosymbolic AI. This hybrid approach combines the pattern recognition capabilities of neural networks with the logical reasoning of symbolic AI. Unlike LLMs, which generate text based on statistical probabilities, neurosymbolic AI systems are designed to truly understand and reason through complex problems. This could enable AI to move beyond merely mimicking human language and into the realm of true problem-solving and critical thinking.</p>
<p>[<a href="https://finance.yahoo.com/news/next-wave-ai-won-t-100327696.html]">https://finance.yahoo.com/news/next-wave-ai-won-t-100327696.html]</a></p>
<h3 id="editors-note-4"><a class="header-link" href="#editors-note-4"></a>Editor&#39;s Note</h3>
<p>It is pretty certain LLMs are not the end of AI models. There will be more and more different model types coming out in coming years and decades. We are just at the beginning of intelligence revolution. </p>
    </article>
  </body>
</html>
