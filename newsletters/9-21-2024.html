<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, minimal-ui">
    <title>Xu&#x27;s AI Newsletter Sep 21, 2024</title>
    <link type="text/css" rel="stylesheet" href="assets/css/github-markdown.css">
    <link type="text/css" rel="stylesheet" href="assets/css/pilcrow.css">
    <link type="text/css" rel="stylesheet" href="assets/css/hljs-github.min.css"/>
  </head>
  <body>
    <article class="markdown-body"><h1 id="xus-ai-newsletter-sep-21-2024"><a class="header-link" href="#xus-ai-newsletter-sep-21-2024"></a>Xu&#39;s AI Newsletter Sep 21, 2024</h1>
<h2 id="omnipresent-ai-cameras-will-ensure-good-behavior-says-larry-ellison"><a class="header-link" href="#omnipresent-ai-cameras-will-ensure-good-behavior-says-larry-ellison"></a>Omnipresent AI cameras will ensure good behavior, says Larry Ellison</h2>
<p>On Thursday, Oracle co-founder Larry Ellison shared his vision for an AI-powered surveillance future during a company financial meeting, reports Business Insider. During an investor Q&amp;A, Ellison described a world where artificial intelligence systems would constantly monitor citizens through an extensive network of cameras and drones, stating this would ensure both police and citizens don&#39;t break the law.</p>
<p>Ellison, who briefly became the world&#39;s second-wealthiest person last week when his net worth surpassed Jeff Bezos&#39; for a short time, outlined a scenario where AI models would analyze footage from security cameras, police body cams, doorbell cameras, and vehicle dash cams.</p>
<p><a href="https://arstechnica.com/information-technology/2024/09/omnipresent-ai-cameras-will-ensure-good-behavior-says-larry-ellison/">https://arstechnica.com/information-technology/2024/09/omnipresent-ai-cameras-will-ensure-good-behavior-says-larry-ellison/</a></p>
<h3 id="editors-note"><a class="header-link" href="#editors-note"></a>Editor&#39;s Note</h3>
<p>Seems like Mr. Ellison likes police state. Could AI pose real danger to human liberty in the end?</p>
<h2 id="supermaven-nabs-12m-for-its-ai-coding-assistant"><a class="header-link" href="#supermaven-nabs-12m-for-its-ai-coding-assistant"></a>Supermaven nabs $12M for its AI coding assistant</h2>
<p>Supermaven Inc., a startup with an artificial intelligence coding assistant of the same name, today announced that it has closed a $12 million funding round led by Bessemer.</p>
<p>The investment also included contributions from several angel investors. The group included OpenAI co-founder John Schulman, Perplexity AI Inc. co-founder Denis Yarats and Intercom Inc. co-founder Eoghan McCabe.</p>
<p>Supermaven distributes its AI assistant in the form of an extension for popular code editors, the applications that developers use to write software. The assistant can analyze the snippet of code that an engineer is writing and generate autocomplete suggestions. Moreover, it’s capable of generating new snippets from scratch based on natural language instructions from the user. </p>
<p><a href="https://siliconangle.com/2024/09/16/supermaven-nabs-12m-ai-coding-assistant/">https://siliconangle.com/2024/09/16/supermaven-nabs-12m-ai-coding-assistant/</a></p>
<h3 id="editors-note-1"><a class="header-link" href="#editors-note-1"></a>Editor&#39;s Note</h3>
<p>Specialized AI based solution will be the norm for lots of AI start ups. </p>
<h2 id="openais-strawberry-thought-process-sometimes-shows-it-scheming-to-trick-users"><a class="header-link" href="#openais-strawberry-thought-process-sometimes-shows-it-scheming-to-trick-users"></a>OpenAI&#39;s Strawberry &quot;Thought Process&quot; Sometimes Shows It Scheming to Trick Users</h2>
<p>ChatGPT maker OpenAI recently released its latest AI model, previously codenamed &quot;Strawberry.&quot;</p>
<p>The model — now saddled with the forgettable moniker of &quot;o1-preview&quot; — is designed to &quot;spend more time thinking&quot; before responding, with OpenAI claiming that it&#39;s capable of &quot;reasoning&quot; through &quot;complex tasks&quot; and solving &quot;harder problems.&quot;</p>
<p>But those capabilities might also make it an exceptionally good liar, as Vox reports. In its system card, essentially a report card for its latest AI model, OpenAI gave o1 a &quot;medium risk&quot; rating in a variety of areas, including persuasion.</p>
<p>In other words, it can use its reasoning skills to deceive users. And ironically, it&#39;ll happily run you through its own &quot;thought&quot; process while coming up with its next scheme.</p>
<p><a href="https://futurism.com/openai-strawberry-thought-process-scheming">https://futurism.com/openai-strawberry-thought-process-scheming</a></p>
<h3 id="editors-note-2"><a class="header-link" href="#editors-note-2"></a>Editor&#39;s NOte</h3>
<p>When faced with convincing arguments, most of people will treat the stated as truth. This will become a bigger problem in the era of AI. It is hard to ignore this risk. </p>
<h2 id="google-seeks-authenticity-in-the-age-of-ai-with-new-content-labeling-system"><a class="header-link" href="#google-seeks-authenticity-in-the-age-of-ai-with-new-content-labeling-system"></a>Google seeks authenticity in the age of AI with new content labeling system</h2>
<p>On Tuesday, Google announced plans to implement content authentication technology across its products to help users distinguish between human-created and AI-generated images. Over several upcoming months, the tech giant will integrate the Coalition for Content Provenance and Authenticity (C2PA) standard, a system designed to track the origin and editing history of digital content, into its search, ads, and potentially YouTube services. However, it&#39;s an open question of whether a technological solution can address the ancient social issue of trust in recorded media produced by strangers.</p>
<p>A group of tech companies created the C2PA system beginning in 2019 in an attempt to combat misleading, realistic synthetic media online. As AI-generated content becomes more prevalent and realistic, experts have worried that it may be difficult for users to determine the authenticity of images they encounter. The C2PA standard creates a digital trail for content, backed by an online signing authority, that includes metadata information about where images originate and how they&#39;ve been modified.</p>
<p><a href="https://arstechnica.com/information-technology/2024/09/google-seeks-authenticity-in-the-age-of-ai-with-new-content-labeling-system/">https://arstechnica.com/information-technology/2024/09/google-seeks-authenticity-in-the-age-of-ai-with-new-content-labeling-system/</a></p>
<h3 id="editors-note-3"><a class="header-link" href="#editors-note-3"></a>Editor&#39;s Note</h3>
<p>Content authenticity is key to successful use of AI generated content in a safe way. This is an important initiative and you can check out details here [<a href="https://c2pa.org/]">https://c2pa.org/]</a></p>
<h2 id="openai-threatens-to-ban-users-who-probe-its-strawberry-ai-models"><a class="header-link" href="#openai-threatens-to-ban-users-who-probe-its-strawberry-ai-models"></a>OpenAI Threatens to Ban Users Who Probe Its ‘Strawberry’ AI Models</h2>
<p>OpenAI truly does not want you to know what its latest AI model is “thinking.” Since the company launched its “Strawberry” AI model family last week, touting so-called reasoning abilities with o1-preview and o1-mini, OpenAI has been sending out warning emails and threats of bans to any user who tries to probe how the model works.</p>
<p>Unlike previous AI models from OpenAI, such as GPT-4o, the company trained o1 specifically to work through a step-by-step problem-solving process before generating an answer. When users ask an &quot;o1&quot; model a question in ChatGPT, users have the option of seeing this chain-of-thought process written out in the ChatGPT interface. However, by design, OpenAI hides the raw chain of thought from users, instead presenting a filtered interpretation created by a second AI model.</p>
<p><a href="https://www.wired.com/story/openai-threatens-bans-as-users-probe-o1-model/">https://www.wired.com/story/openai-threatens-bans-as-users-probe-o1-model/</a></p>
<h3 id="editors-note-4"><a class="header-link" href="#editors-note-4"></a>Editor&#39;s Note</h3>
<p>Another reason that OpenAI should change its name to ClosedAI which aligns to its value better</p>
<h2 id="mistral-launches-a-free-tier-for-developers-to-test-its-ai-models"><a class="header-link" href="#mistral-launches-a-free-tier-for-developers-to-test-its-ai-models"></a>Mistral launches a free tier for developers to test its AI models</h2>
<p>Mistral AI launched a new free tier to let developers fine-tune and build test apps with the startup’s AI models, the company announced in a blog post Tuesday. The startup also slashed prices for developers to access its AI models through API endpoints and added image processing to its free consumer AI chatbot, le Chat.</p>
<p>There’s a growing trend in the AI model provider world: offering more for less. The Paris-based startup, which was recently valued at $6 billion, announced several updates on Tuesday to attract developers. Advanced large language models (LLMs) are becoming commoditized in the developer world, and Mistral is trying to compete with plummeting prices for developers from OpenAI, Google, and Anthropic.</p>
<p><a href="https://techcrunch.com/2024/09/17/mistral-launches-a-free-tier-for-developers-to-test-its-ai-models/">https://techcrunch.com/2024/09/17/mistral-launches-a-free-tier-for-developers-to-test-its-ai-models/</a></p>
<h3 id="editors-note-5"><a class="header-link" href="#editors-note-5"></a>Editor&#39;s Note</h3>
<p>Free is always good. :-) I would like to give it a try. </p>
<h2 id="ai-fastest-growing-technology-weve-seen-in-the-history-of-our-company"><a class="header-link" href="#ai-fastest-growing-technology-weve-seen-in-the-history-of-our-company"></a>AI ‘Fastest-Growing Technology We’ve Seen In The History Of Our Company’</h2>
<p>There have been many questions arising around the potential returns seen from artificial intelligence and generative AI. As we reported here last month, there is growing nervousness among board-level leaders concerned that their organizations are diving too quickly into AI without weighing its risks, such as inaccuracies and workplace disruptions. Have we’ve been led to expect too much too soon?</p>
<p>Recent research out of Accenture suggests that generative AI is delivering on its promises. The consultancy’s survey of 2,800 C-suite executives finds that 49% of C-suite leaders say their gen AI investments are outperforming other investments.</p>
<p>“It is the fastest-growing technology we’ve seen in the history of our company.” Jack Azagury, group chief executive for consulting at Accenture, told me. The numbers are all positive, but at the same time, AI’s main weakness is lack of a “digital core,” he cautioned.</p>
<p><a href="https://www.forbes.com/sites/joemckendrick/2024/09/19/ai-fastest-growing-technology-weve-seen-in-the-history-of-our-company/">https://www.forbes.com/sites/joemckendrick/2024/09/19/ai-fastest-growing-technology-weve-seen-in-the-history-of-our-company/</a></p>
<h3 id="editors-note-6"><a class="header-link" href="#editors-note-6"></a>Editor&#39;s Note</h3>
<p>Not a hype. at least not yet?</p>
    </article>
  </body>
</html>
